{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data preparation package\n",
    "from azureml.dataprep import package\n",
    "\n",
    "# This call will load the referenced package and return a DataFrame.\n",
    "# If run in a PySpark environment, this call returns a\n",
    "# Spark DataFrame. If not, it returns a Pandas DataFrame.\n",
    "df = package.run('dataPrepPkg.dprep', dataflow_idx=0)\n",
    "\n",
    "# Remove this line and add code that uses the DataFrame\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "%azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped = df.na.fill(0)\n",
    "display(prepped.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of our data is prepped. We're going to have to put all of it into one column of a vector type for Spark MLLib. This makes it easy to embed a prediction right in a DataFrame and also makes it very clear as to what is getting passed into the model and what isn't without have to convert it to a numpy array or specify an R formula. This also makes it easy to incrementally add new features, simply by adding to the vector. In the below case rather than specifically adding them in, I'm going to create a exclusionary group and just remove what is NOT a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonFeatureCols = [\"zip\", \"zipcode\", \"count\"]\n",
    "featureCols = list(set(prepped.columns)-set(nonFeatureCols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md Now I'm going to use the `VectorAssembler` in Apache Spark to Assemble all of these columns into one single vector. To do this I'll have to set the input columns and output column. Then I'll use that assembler to transform the prepped data to my final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\\\n",
    "  inputCols =featureCols\\\n",
    "  ,outputCol = \"features\")\n",
    "\n",
    "finalPrep = assembler.transform(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = finalPrep.randomSplit([0.7, 0.3])\n",
    "\n",
    "# // Going to cache the data to make sure things stay snappy!\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "print(training.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lrModel = LinearRegression(\\\n",
    "  labelCol = \"count\"\\\n",
    "  ,featuresCol = \"features\"\\\n",
    "  ,elasticNetParam =0.5)\n",
    "\n",
    "print(\"Printing out the model Parameters:\")\n",
    "print(\"-\"*20)\n",
    "print(lrModel.explainParams())\n",
    "logger.log(\"modelParams\",lrModel.explainParams())\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "lrFitted = lrModel.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = lrFitted\\\n",
    "  .transform(test)\\\n",
    "  .selectExpr(\"prediction as raw_prediction\", \n",
    "    \"double(round(prediction)) as prediction\", \n",
    "    \"count\", \n",
    "    \"\"\"CASE double(round(prediction)) = count \n",
    "  WHEN true then 1\n",
    "  ELSE 0\n",
    "END as equal\"\"\")\n",
    "display(holdout.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(holdout.selectExpr(\"sum(equal)/sum(1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // have to do a type conversion for RegressionMetric\n",
    "rm = RegressionMetrics(holdout.select(\"prediction\", \"count\").rdd.map(lambda x: (x[0], x[1])))\n",
    "\n",
    "print(\"MSE: \" + str(rm.meanSquaredError))\n",
    "logger.log(\"MSE\",str(rm.meanSquaredError))\n",
    "print(\"MAE: \" + str(rm.meanAbsoluteError))\n",
    "logger.log(\"\")\n",
    "print(\"RMSE Squared: \" + str(rm.rootMeanSquaredError))\n",
    "logger.log(\"\")\n",
    "print(\"R Squared: \" + str(rm.r2))\n",
    "logger.log(\"\")\n",
    "print(\"Explained Variance: \" + str(rm.explainedVariance) + \"\\n\")\n",
    "logger.log(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%azureml history off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_playgroung docker",
   "language": "python",
   "name": "aml_playgroung_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
