# Exercise 

## Create pySpark Environment

1. Create new project with MMLSpark on Audit Census template.

2. Create pySpark environment on your local machine with Docker.

3. Open python IDE.

4. Wirte python code and do experiment.

## Experiment includes

### Access data

Download sample data in a blob. You can use python or data source to access it.

### Experiments

Run experiment and find model.

```python
# This PySpark code uses regular Spark ML constructs. 
# It is a lot more involved comparing to the MMLSpark version.

import numpy as np
import pandas as pd
import pyspark
import os,urllib
import requests
from azureml.logging import get_azureml_logger

# Initialize the logger
run_logger = get_azureml_logger() 

# Start Spark application
spark = pyspark.sql.SparkSession.builder.getOrCreate()

# Download joined_dataset.csv from blob
dataFilePath = "joined_dataset.csv"
if not os.path.isfile(dataFilePath):
    urllib.request.urlretrieve("https://mtcsdataai.blob.core.windows.net/taxandmarket/" + dataFilePath, dataFilePath)

# Create a Spark dataframe out of the csv file.
joined = spark.createDataFrame(pd.read_csv(dataFilePath))
print(joined.limit(5).toPandas())

prepped = joined.na.fill(0)

# Choose a few relevant columns and the label column.
nonFeatureCols = ["zip", "zipcode", "count"]
featureCols = list(set(prepped.columns)-set(nonFeatureCols))

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(\
  inputCols =featureCols\
  ,outputCol = "features")

finalPrep = assembler.transform(prepped)

training, test = finalPrep.randomSplit([0.7, 0.3])

# // Going to cache the data to make sure things stay snappy!
training.cache()
test.cache()

print(training.count())
print(test.count())

# change regularization rate and you will likely get a different accuracy.
reg = 0.5
# load regularization rate from argument if present
if len(sys.argv) > 1:
    reg = float(sys.argv[1])

print("Regularization rate is {}".format(reg))

from pyspark.ml.regression import LinearRegression
lrModel = LinearRegression(\
  labelCol = "count"\
  ,featuresCol = "features"\
  ,regParam= reg\
  ,elasticNetParam =0.5)

print("Printing out the model Parameters:")
print("-"*20)
print(lrModel.explainParams())
run_logger.log("modelParams",lrModel.explainParams())
print("-"*20)

from pyspark.mllib.evaluation import RegressionMetrics

lrFitted = lrModel.fit(training)

holdout = lrFitted\
  .transform(test)\
  .selectExpr("prediction as raw_prediction", 
    "double(round(prediction)) as prediction", 
    "count", 
    """CASE double(round(prediction)) = count 
  WHEN true then 1
  ELSE 0
END as equal""")
print(holdout.limit(5).toPandas())

print(holdout.selectExpr("sum(equal)/sum(1)"))

# // have to do a type conversion for RegressionMetric
rm = RegressionMetrics(holdout.select("prediction", "count").rdd.map(lambda x: (x[0], x[1])))

print("MSE: " + str(rm.meanSquaredError))
run_logger.log("MSE",str(rm.meanSquaredError))
print("MAE: " + str(rm.meanAbsoluteError))
run_logger.log("MAE: ", str(rm.meanAbsoluteError))
print("RMSE Squared: " + str(rm.rootMeanSquaredError))
run_logger.log("RMSE Squared: ", str(rm.rootMeanSquaredError))
print("R Squared: " + str(rm.r2))
run_logger.log("R Squared: ", str(rm.r2))
print("Explained Variance: " + str(rm.explainedVariance) + "\n")
run_logger.log("Explained Variance: ", str(rm.explainedVariance))

```
